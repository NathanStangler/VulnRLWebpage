<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2025 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Compiler-Verified RL for Code Vulnerability Detection</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2025 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Embedding Squad</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <p>
                        
              Nathan Stangler
            
          </p>
        </div>
        
        <div class="author-container">
          <p>
            
            Tamojit Bera
            
          </p>
        </div>
        
        <div class="author-container">
          <p>
              Aryan Jaiswal
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!--
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          -->
          <span class="link-block">
            <a
              href="https://github.com/NathanStangler/VulnRL"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>  
          <!--    
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>
          -->           
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

<p>Software vulnerabilities remain a critical threat to modern systems, with existing detection tools often suffering from limited generalization, inconsistent coverage, and high false-positive rates. This proposal introduces a <b>compiler-verified reinforcement learning framework</b> for automated vulnerability detection in source code. Our approach leverages <em>compiler feedback</em>—including error diagnostics, static analyzers, and sanitizers—as dynamic reward signals to guide model improvement, moving beyond static benchmark-based evaluation. Unlike traditional compiler-based methods that require exhaustively compiling large codebases, our model-based approach learns vulnerability patterns directly, enabling faster and more scalable detection. The proposed system integrates reinforcement learning with parameter-efficient fine-tuning to adapt across programming languages and Common Weakness Enumeration (CWE) categories. We outline our experimental design, resource plan, and evaluation strategy to demonstrate how reinforcement learning, guided by compiler and analyzer signals, can yield a more accurate and computationally efficient solution for secure software development.</p>

<hr>

<h2 id="teaser">Compiler-Verified Reinforcement Learning</h2>

<p class="sys-img"><img src="./files/diagram.png" alt="RL Figure"></p>

<p>Source code samples are fed into a fine-tuned coder model that predict vulnerabilities. The model receives feedback from the compiler and analyzers to improve its predictions.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
We created a machine learning model that detects security vulnerabilities in code. Current tools miss many vulnerabilities. Instead of only checking for known bad patterns, the model learns from feedback from compiler errors, warnings, and sanitizer messages, which better identifies unsafe code. Our goal was to create a model that uses compiler feedback to detect unsafe code more accurately and quickly.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
Today, developers use static analyzers and testing tools that look for known patterns. These tools work well for common vulnerabilities, but are limited because they rely on set patterns and do not adapt to new kinds of vulnerabilities. They also generate many false positives and false negatives, and they do not learn from the runtime of code. This causes coders to waste time chasing false warnings or miss real security problems.
</p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
If successful, the model could make software development safer and more efficient. Developers would spend less time dealing with code vulnerabilities and reduce the number of security issues.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
We developed a reinforcement learning model designed to detect software vulnerabilities, and we trained it using feedback from compilers and static analyzers. Unlike traditional approaches that rely solely on prelabeled datasets or test cases, our model receives multiple types of feedback signals from the code it analyzes. These signals include whether the code successfully compiles, whether it triggers compiler warnings, and whether it causes runtime errors. Adding these signals into the training loop helps the model understand what causes a vulnerability. The model receives a positive reward when code passes compilation without errors or warnings, while negative rewards are given when runtime errors or potential vulnerabilities are detected. This approach allows the model to learn from the security tools rather than simply memorizing patterns in labeled datasets. Over time, the model develops a nuanced understanding of both safe and unsafe code patterns, improving its ability to generalize to unseen codebases and novel vulnerabilities.

Existing vulnerability detection tools and static analyzers struggle with generalization and have high false-positive rates because they rely on limited examples and do not validate whether code actually compiles or executes correctly. By integrating compiler and analyzer feedback into the training process, our approach provides the model with more accurate information about code security. We expected this method would outperform traditional approaches because it combines the strengths of program verification with reinforcement learning, which creates a model that continuously improves.

The novelty of our approach is the use of compiler verification and static analyzers with a reinforcement learning model. Previous work has used machine learning to detect vulnerabilities or generate code, but most models are trained purely on labeled datasets without verifying whether their outputs are correct. By combining RL with compiler and analyzer feedback, we create a model receives information about its performance. This approach allows it to generalize better to unseen code and continuously improve over time.</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
We expected issues with the compiler tools giving too little feedback, and difficulty training on long pieces of code. The issue with long pieces of code was handled using LlamaIndex to chunk the code into smaller pieces that are split based on logical sections, like functions or classes. The chunks allowed the model to classify larger code samples. However, instead of having issues with too little feedback, we had issues with the compiler tools giving too much feedback. The compiler and analyzers alerted of issues that did not actually cause vulnerabilities, which led to high false-positives. The initial attempts to correct these issues did not work well. However, with some extra improvements to the reward value, we were able to get the model performing slightly better than baseline finetuning.
</p>

<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
We evaluated the model's performance using standard metrics like accuracy, precision, recall, and F1-score on the test dataset for ten epochs of fine-tuning the Qwen2.5-Coder-1.5B-Instruct model without reinforcement learning. The metrics are for the truncated and chunked code samples, and for Common Weakness Evaluation category or safe/unsafe.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Experiment</strong></th>
      <th style="text-align: center">Accuracy</th>
      <th style="text-align: center">Precision</th>
      <th style="text-align: center">Recall</th>
      <th style="text-align: center">F1-score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Truncated</strong></td>
      <td style="text-align: center">0.1948</td>
      <td style="text-align: center">0.2024</td>
      <td style="text-align: center">0.1948</td>
      <td style="text-align: center">0.1921</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chunked</strong></td>
      <td style="text-align: center">0.2078</td>
      <td style="text-align: center">0.2445</td>
      <td style="text-align: center">0.2078</td>
      <td style="text-align: center">0.2246</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Truncated Binary</strong></td>
      <td style="text-align: center">0.3766</td>
      <td style="text-align: center">0.3889</td>
      <td style="text-align: center">0.3500</td>
      <td style="text-align: center">0.3684</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Chunked Binary</strong></td>
      <td style="text-align: center">0.4545</td>
      <td style="text-align: center">0.4706</td>
      <td style="text-align: center">0.4000</td>
      <td style="text-align: center">0.4324</td>
    </tr>
  </tbody>
  <caption>Table 1. Performance metrics on the fine-tuned Qwen2.5-Coder-1.5B-Instruct model without reinforcement learning.</caption>
</table>

<p>We evaluated the compiler-verified RL model (Qwen 2.5 Coder 1.5B Instruct) on a C++ vulnerability dataset and compared it against a supervised finetuning baseline. Compiler-driven reward signals improved detection performance and generalization. RL training reduced several categories of false negatives by leveraging sanitizer and Clang-Tidy failures as corrective signals, while false-positive rates remain influenced by security tools' over-detection.</p>

<table>
  <thead>
    <tr>
      <th style="text-align:center">Model</th>
      <th style="text-align:center">Accuracy</th>
      <th style="text-align:center">F1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align:center">Finetuned</td>
      <td style="text-align:center">0.1948</td>
      <td style="text-align:center">0.1921</td>
    </tr>
    <tr>
      <td style="text-align:center">RL</td>
      <td style="text-align:center">0.2317</td>
      <td style="text-align:center">0.2651</td>
    </tr>
  </tbody>
  <caption>Table 2. Performance comparison between finetuned and RL model.</caption>
</table>

<p>We created a UI that allows users to interact with the model, view its outputs, and see feedback from the compiler and analyzer tools. The UI makes it simple for users to access the model and detect vulnerabilities within their code.</p>
<img src="./files/ui.png" alt="UI Figure">
<hr>

<h2 id="conclusion">Conclusion and Future Work</h2>
<p>

  How easily are your results able to be reproduced by others?
  Did your dataset or annotation affect other people's choice of research or development projects to undertake?
  Does your work have potential harm or risk to our society? What kinds? If so, how can you address them?
  What limitations does your model have? How can you extend your work for future research?</p>

<p>
Our experiments show that compiler-verified reinforcement learning delivers stronger performance than standard finetuning. The results are able to be reproduced with different models and datasets using the simple configurable scripts provided in the code. RL leverages execution and compiler signals to reduce some failures, although the approach remains limited by the capabilities of the underlying base model and by false-positive over-detection from security tools, which could influence reproducibility depending on the specific datasets and tools used. Our approach provides a method for using reinforcement learning on multiple datasets and models, which can be expanded by other researchers or developers in similar approaches or extending RL-based security detection methods. One potential risk from vulnerability detecting machine learning models could include over-reliance on the model's predictions and possible misclassification of code as malicious, which could affect software development or security decisions. These risks can be reduced by combining automated detection with human review and by continuing to improve the model's performance. The main limitations of our model are its dependence on the base model's capabilities and the over-detection from security tools. Future research can extend this work by evaluating additional base models, using broader assessment datasets, and refining the reward policy to further strengthen detection accuracy.
</p>

<hr>


</div>
  


</body></html>
